{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "#tf.disable_v2_behavior() \n",
        "tf.compat.v1.disable_v2_behavior()"
      ],
      "metadata": {
        "id": "wtR2X58Yj1Hi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloader\n",
        "import time\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def read_all_files(out_file, *filenames):\n",
        "    '''if you have few files into your directory, then use this function and compare them into one file, which name you wright as 'oufile'\n",
        "    read_all_files('randome_name.txt', 'l.txt')\n",
        "    '''\n",
        "    with open(out_file, 'w', encoding=\"utf8\") as outfile:\n",
        "        for fname in filenames:\n",
        "            with open(fname, encoding='utf8') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)\n",
        "    return outfile\n",
        "\n",
        "def only_kirrilica(fin_filename, file_with_big_letters_file_name):\n",
        "    alphabet = (' АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя')\n",
        "    fin = open(fin_filename, 'r', encoding=\"utf-8\")\n",
        "    file_with_big_letters = open(file_with_big_letters_file_name, \"wt\", encoding=\"utf-8\")\n",
        "    for letter in fin.read():\n",
        "        if letter in alphabet:\n",
        "            file_with_big_letters.write(letter)\n",
        "    return file_with_big_letters\n",
        "\n",
        "\n",
        "def lowercase_txt(file_name):\n",
        "    \"\"\"\n",
        "    file_name is the full path to the file to be opened\n",
        "    \"\"\"\n",
        "    with open(file_name, 'r', encoding=\"utf8\") as f:\n",
        "        contents = f.read()  # read contents of file\n",
        "    contents = contents.lower()  # convert to lower case\n",
        "    with open(file_name, 'w', encoding=\"utf8\") as f:  # open for output\n",
        "        f.write(contents)\n",
        "\n",
        "read_all_files('result.txt', 'input_file1.txt', 'inputfile2.txt')\n",
        "only_kirrilica('result.txt', 'result_with_ru.txt')\n",
        "lowercase_txt('result_with_ru.txt')"
      ],
      "metadata": {
        "id": "tE2tzE5oTlOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/result_with_ru.txt', 'r') as f:\n",
        "    text=f.read()\n",
        "vocab = sorted(set(text))\n",
        "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
      ],
      "metadata": {
        "id": "Jw_IHgkbj9d3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxRzlNRlnEw7",
        "outputId": "1eeae6f3-1669-4378-bb91-fc745f873f48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tGwFgSlsnEh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, n_seqs, n_steps):\n",
        "    '''Создаем генератор, который возвращает пакеты размером\n",
        "       n_seqs x n_steps из массива arr.\n",
        "       \n",
        "       Аргументы\n",
        "       ---------\n",
        "       arr: Массив, из которого получаем пакеты\n",
        "       n_seqs: Batch size, количество последовательностей в пакете\n",
        "       n_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
        "    '''\n",
        "    # Считаем количество символов на пакет и количество пакетов, которое можем сформировать\n",
        "    characters_per_batch = n_seqs * n_steps\n",
        "    n_batches = len(arr)//characters_per_batch\n",
        "    \n",
        "    # Сохраняем в массиве только символы, которые позволяют сформировать целое число пакетов\n",
        "    arr = arr[:n_batches * characters_per_batch]\n",
        "    \n",
        "    # Делаем reshape 1D -> 2D, используя n_seqs как число строк, как на картинке\n",
        "    arr = arr.reshape((n_seqs, -1))\n",
        "    \n",
        "    for n in range(0, arr.shape[1], n_steps):\n",
        "        # пакет данных, который будет подаваться на вход сети\n",
        "        x = arr[:, n:n+n_steps]\n",
        "        # целевой пакет, с которым будем сравнивать предсказание, получаем сдвиганием \"x\" на один символ вперед\n",
        "        y = np.zeros_like(x)\n",
        "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
        "        yield x, y"
      ],
      "metadata": {
        "id": "27qHWgZZkBBh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.9.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WwVcxJD49x5n",
        "outputId": "0b4cf72e-e3bb-467e-e858-a0fc6de0e981"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.0 kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 49.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (21.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (4.1.1)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.47.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (57.4.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (0.26.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.15.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (14.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.9.1) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow==2.9.1) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0.7\n",
            "    Uninstalling flatbuffers-2.0.7:\n",
            "      Successfully uninstalled flatbuffers-2.0.7\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "flatbuffers",
                  "gast",
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batches = get_batches(encoded, 10, 50)\n",
        "x, y = next(batches)\n",
        "print('x\\n', x[:5, :5])\n",
        "print('\\ny\\n', y[:5, :5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCtVAXbkkDql",
        "outputId": "8980af8c-443d-4d1a-d831-5c8ef6882056"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[ 3  0 11 15 14]\n",
            " [15  0 18  0 20]\n",
            " [13  0  8  1 16]\n",
            " [ 1  0 18  6 10]\n",
            " [15  3  6 11  0]]\n",
            "\n",
            "y\n",
            " [[ 0 11 15 14 23]\n",
            " [ 0 18  0 20 18]\n",
            " [ 0  8  1 16  6]\n",
            " [ 0 18  6 10 24]\n",
            " [ 3  6 11  0 18]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inputs(batch_size, num_steps):\n",
        "    ''' Определяем placeholder'ы для входных, целевых данных, а также вероятности drop out\n",
        "    \n",
        "        Аргументы\n",
        "        ---------\n",
        "        batch_size: Batch size, количество последовательностей в пакете\n",
        "        num_steps: Sequence length, сколько \"шагов\" делаем в пакете\n",
        "        \n",
        "    '''\n",
        "    # Объявляем placeholder'ы\n",
        "    inputs = tf.compat.v2.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
        "    targets = tf.compat.v2.placeholder(tf.int32, [batch_size, num_steps], name='targets')\n",
        "    \n",
        "    # Placeholder для вероятности drop out\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    return inputs, targets, keep_prob"
      ],
      "metadata": {
        "id": "c7ha5PldkGSz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
        "    ''' Строим LSTM ячейку.\n",
        "    \n",
        "        Аргументы\n",
        "        ---------\n",
        "        keep_prob: Скаляр (tf.placeholder) для dropout keep probability\n",
        "        lstm_size: Размер скрытых слоев в LSTM ячейках\n",
        "        num_layers: Количество LSTM слоев\n",
        "        batch_size: Batch size\n",
        "\n",
        "    '''\n",
        "    ### Строим LSTM ячейку\n",
        "    \n",
        "    def build_cell(lstm_size, keep_prob):\n",
        "        # Начинаем с базовой LSTM ячейки\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "        \n",
        "        # Добавляем dropout к ячейке\n",
        "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
        "        return drop\n",
        "    \n",
        "    \n",
        "    # Стэкируем несколько LSTM слоев для придания глубины нашему deep learning\n",
        "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
        "    # Инициализируем начальное состояние LTSM ячейки\n",
        "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "    return cell, initial_state"
      ],
      "metadata": {
        "id": "dNfCzlqckItt"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_output(lstm_output, in_size, out_size):\n",
        "    ''' Строим softmax слой и возвращаем результат его работы.\n",
        "    \n",
        "        Аргументы\n",
        "        ---------\n",
        "        \n",
        "        x: Входящий от LSTM тензор\n",
        "        in_size: Размер входящего тензора, (кол-во LSTM юнитов скрытого слоя)\n",
        "        out_size: Размер softmax слоя (объем словаря)\n",
        "    \n",
        "    '''\n",
        "\n",
        "    # вытягиваем и решэйпим тензор, выполняя преобразование 3D -> 2D\n",
        "    seq_output = tf.concat(lstm_output, axis=1)\n",
        "    x = tf.reshape(seq_output, [-1, in_size])\n",
        "    \n",
        "    # Соединяем результат LTSM слоев с softmax слоем\n",
        "    with tf.variable_scope('softmax'):\n",
        "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
        "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
        "    \n",
        "    # Считаем logit-функцию\n",
        "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
        "    # Используем функцию softmax для получения предсказания\n",
        "    out = tf.nn.softmax(logits, name='predictions')\n",
        "    \n",
        "    return out, logits"
      ],
      "metadata": {
        "id": "SyDKzO77kKzU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_output(lstm_output, in_size, out_size):\n",
        "    ''' Строим softmax слой и возвращаем результат его работы.\n",
        "    \n",
        "        Аргументы\n",
        "        ---------\n",
        "        \n",
        "        x: Входящий от LSTM тензор\n",
        "        in_size: Размер входящего тензора, (кол-во LSTM юнитов скрытого слоя)\n",
        "        out_size: Размер softmax слоя (объем словаря)\n",
        "    \n",
        "    '''\n",
        "\n",
        "    # вытягиваем и решэйпим тензор, выполняя преобразование 3D -> 2D\n",
        "    seq_output = tf.concat(lstm_output, axis=1)\n",
        "    x = tf.reshape(seq_output, [-1, in_size])\n",
        "    \n",
        "    # Соединяем результат LTSM слоев с softmax слоем\n",
        "    with tf.variable_scope('softmax'):\n",
        "        softmax_w = tf.Variable(tf.truncated_normal((in_size, out_size), stddev=0.1))\n",
        "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
        "    \n",
        "    # Считаем logit-функцию\n",
        "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
        "    # Используем функцию softmax для получения предсказания\n",
        "    out = tf.nn.softmax(logits, name='predictions')\n",
        "    \n",
        "    return out, logits"
      ],
      "metadata": {
        "id": "4fajgX3CkMkm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_optimizer(loss, learning_rate, grad_clip):\n",
        "    ''' Строим оптимизатор для обучения, используя обрезку градиента.\n",
        "    \n",
        "        Arguments:\n",
        "        loss: значение функции потери\n",
        "        learning_rate: параметр скорости обучения\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    # Оптимизатор для обучения, обрезка градиента для контроля \"взрывающихся\" градиентов\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
        "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
        "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
        "    \n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "j4lveM78kOKj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN:\n",
        "    \n",
        "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
        "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
        "                       grad_clip=5, sampling=False):\n",
        "    \n",
        "        # Мы будем использовать эту же сеть для сэмплирования (генерации текста),\n",
        "        # при этом будем подавать по одному символу за один раз\n",
        "        if sampling == True:\n",
        "            batch_size, num_steps = 1, 1\n",
        "        else:\n",
        "            batch_size, num_steps = batch_size, num_steps\n",
        "\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "        \n",
        "        # Получаем input placeholder'ы\n",
        "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
        "\n",
        "        # Строим LSTM ячейку\n",
        "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
        "\n",
        "        ### Прогоняем данные через RNN слои\n",
        "        # Делаем one-hot кодирование входящих данных\n",
        "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
        "        \n",
        "        # Прогоняем данные через RNN и собираем результаты\n",
        "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
        "        self.final_state = state\n",
        "        \n",
        "        # Получаем предсказания (softmax) и результат logit-функции\n",
        "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
        "        \n",
        "        # Считаем потери и оптимизируем (с обрезкой градиента)\n",
        "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
        "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
      ],
      "metadata": {
        "id": "Xq3__6W-kQyO"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100        # Размер пакета\n",
        "num_steps = 100         # Шагов в пакете\n",
        "lstm_size = 512         # Количество LSTM юнитов в скрытом слое\n",
        "num_layers = 2          # Количество LSTM слоев\n",
        "learning_rate = 0.001   # Скорость обучения\n",
        "keep_prob = 0.5         # Dropout keep probability\n"
      ],
      "metadata": {
        "id": "PnQ3WQUokTH3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "# Сохраняться каждый N итераций\n",
        "save_every_n = 200\n",
        "\n",
        "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
        "                lstm_size=lstm_size, num_layers=num_layers, \n",
        "                learning_rate=learning_rate)\n",
        "\n",
        "saver = tf.train.Saver(max_to_keep=100)\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Можно раскомментировать строчку ниже и продолжить обучение с checkpoint'а\n",
        "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
        "    counter = 0\n",
        "    for e in range(epochs):\n",
        "        # Обучаем сеть\n",
        "        new_state = sess.run(model.initial_state)\n",
        "        loss = 0\n",
        "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
        "            counter += 1\n",
        "            start = time.time()\n",
        "            feed = {model.inputs: x,\n",
        "                    model.targets: y,\n",
        "                    model.keep_prob: keep_prob,\n",
        "                    model.initial_state: new_state}\n",
        "            batch_loss, new_state, _ = sess.run([model.loss, \n",
        "                                                 model.final_state, \n",
        "                                                 model.optimizer], \n",
        "                                                 feed_dict=feed)\n",
        "            \n",
        "            end = time.time()\n",
        "            print('Epoch: {}/{}... '.format(e+1, epochs),\n",
        "                  'Training Step: {}... '.format(counter),\n",
        "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
        "                  '{:.4f} sec/batch'.format((end-start)))\n",
        "        \n",
        "            if (counter % save_every_n == 0):\n",
        "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
        "    \n",
        "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "CtovBcm-kWMH",
        "outputId": "527ad012-018b-467d-e560-cd1a9061daec"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-5c4031272cb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n\u001b[1;32m      6\u001b[0m                 \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 learning_rate=learning_rate)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-a78c6caac33d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clip, sampling)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Получаем input placeholder'ы\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Строим LSTM ячейку\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-cc4e40483c07>\u001b[0m in \u001b[0;36mbuild_inputs\u001b[0;34m(batch_size, num_steps)\u001b[0m\n\u001b[1;32m      9\u001b[0m     '''\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Объявляем placeholder'ы\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2' has no attribute 'placeholder'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.train.get_checkpoint_state('checkpoints')"
      ],
      "metadata": {
        "id": "N9gO6RxMkZII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_top_n(preds, vocab_size, top_n=5):\n",
        "    p = np.squeeze(preds)\n",
        "    p[np.argsort(p)[:-top_n]] = 0\n",
        "    p = p / np.sum(p)\n",
        "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
        "    return c\n"
      ],
      "metadata": {
        "id": "do-qU9yskbFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"Гостиная Анны Павловны начала понемногу наполняться.\"):\n",
        "    samples = [c for c in prime]\n",
        "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
        "    saver = tf.train.Saver()\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, checkpoint)\n",
        "        new_state = sess.run(model.initial_state)\n",
        "        for c in prime:\n",
        "            x = np.zeros((1, 1))\n",
        "            x[0,0] = vocab_to_int[c]\n",
        "            feed = {model.inputs: x,\n",
        "                    model.keep_prob: 1.,\n",
        "                    model.initial_state: new_state}\n",
        "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
        "                                         feed_dict=feed)\n",
        "\n",
        "        c = pick_top_n(preds, len(vocab))\n",
        "        samples.append(int_to_vocab[c])\n",
        "\n",
        "        for i in range(n_samples):\n",
        "            x[0,0] = c\n",
        "            feed = {model.inputs: x,\n",
        "                    model.keep_prob: 1.,\n",
        "                    model.initial_state: new_state}\n",
        "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
        "                                         feed_dict=feed)\n",
        "\n",
        "            c = pick_top_n(preds, len(vocab))\n",
        "            samples.append(int_to_vocab[c])\n",
        "        \n",
        "    return ''.join(samples)"
      ],
      "metadata": {
        "id": "QPR5D6Vckg8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
        "samp = sample(checkpoint, 1000, lstm_size, len(vocab))\n",
        "print(samp)"
      ],
      "metadata": {
        "id": "SzMyIAQDkjLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
        "samp = sample(checkpoint, 1000, lstm_size, len(vocab))\n",
        "print(samp)"
      ],
      "metadata": {
        "id": "-Q2tRQhtkoW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
        "samp = sample(checkpoint, 2000, lstm_size, len(vocab))\n",
        "print(samp)"
      ],
      "metadata": {
        "id": "qd0k22Qykq6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
